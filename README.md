# ğŸ§  LLM App Builder & Deployer

![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi&logoColor=white)
![GitHub Pages](https://img.shields.io/badge/Deploy-GitHub%20Pages-327FC7?style=for-the-badge&logo=github&logoColor=white)
![LLM](https://img.shields.io/badge/Powered%20by-LLM%20(Gemini%20%2F%20GPT)-purple?style=for-the-badge)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

> A **FastAPI-based automation service** that can build, deploy, and update web applications automatically using **LLMs (Gemini / GPT)** and **GitHub Pages**.

It receives a project brief via a POST request, uses an LLM to generate the application code, creates a GitHub repository, deploys it on GitHub Pages, and notifies an evaluation server when complete.

---

## ğŸš€ Features

- ğŸ“¨ **FastAPI endpoint** to receive project briefs (`/api-endpoint`)
- ğŸ” **Secret verification** for authorized requests  
- ğŸ¤– **LLM integration** (Gemini or GPT) for app generation  
- ğŸ§© **Automatic GitHub repo creation & push**
- ğŸŒ **GitHub Pages deployment**
- ğŸ” **Update support** for Round 2 (smart regeneration)
- ğŸ“¬ **Evaluation callback** to instructorâ€™s API with repo and live site details

---

## ğŸ§© Project Structure

ğŸ“¦ TDS_P1_SEP_2025

â”œâ”€â”€ main.py # FastAPI backend for LLM-based app automation

â”œâ”€â”€ requirements.txt # Python dependencies

â”œâ”€â”€ Dockerfile # Container deployment file

â”œâ”€â”€ README.md # Project documentation

â””â”€â”€ .env # Environment variables (excluded from Git)(here txt file to show only)


---

## âš™ï¸ Setup Instructions

1ï¸âƒ£ Clone the Repository

git clone https://github.com/PAVAN2005-LAB/TDS_P1_SEP_2025.git
cd TDS_P1_SEP_2025

2ï¸âƒ£ Install Dependencies
 
pip install -r requirements.txt

3ï¸âƒ£ Create a .env File

Add the following keys:

GEMINI_API_KEY=your_gemini_api_key

GITHUB_TOKEN=your_github_pat

GITHUB_USERNAME=your_github_username

STUDENT_SECRET=your_secret_value


ğŸ’¡ The STUDENT_SECRET should match the one you submitted in the Google Form.

4ï¸âƒ£ Run the API Server
uvicorn main:app --reload --port 8000


The API will run locally at:

ğŸ‘‰ http://127.0.0.1:8000/api-endpoint

To expose it publicly for testing, use:

ngrok http 8000

ğŸ§  Example Request
curl -X POST https://<your-ngrok-url>/api-endpoint \
  -H "Content-Type: application/json" \
  -d '{
    "email": "student@example.com",
    "task": "captcha-solver-123",
    "brief": "Create a simple web app that generates and validates CAPTCHAs.",
    "secret": "your_secret_value",
    "round": 1,
    "nonce": "xyz123",
    "evaluation_url": "https://tds.project-server.edu/eval"
  }'



---
ğŸ§© What Happens Internally

1ï¸âƒ£ The POST request is received and validated.
2ï¸âƒ£ The brief is sent to Gemini / GPT for code generation.
3ï¸âƒ£ Files are stored locally and pushed to a new GitHub repository.
4ï¸âƒ£ GitHub Pages is enabled for live deployment.
5ï¸âƒ£ A callback POST is sent to the evaluation URL containing:

Repo URL

Commit SHA

GitHub Pages URL

---
ğŸ” Round 2 (Update Mode)

When the same task is sent again with "round": 2:

The system pulls the previous repo.

Generates updates using the LLM.

Pushes changes and re-deploys.

Sends an updated evaluation callback.

ğŸ§¾ Example API Responses

From your endpoint:

{
  "status": "accepted"
}


Callback sent to evaluator:

{
  "email": "student@example.com",
  "task": "captcha-solver-123",
  "round": 1,
  "nonce": "xyz123",
  "repo_url": "https://github.com/PAVAN2005-LAB/captcha-solver-123",
  "commit_sha": "d41d8cd9",
  "pages_url": "https://pavan2005-lab.github.io/captcha-solver-123/"
}




----
ğŸ§° Tech Stack
Technology	Purpose
Python 3.10+	Core language
FastAPI	REST backend
httpx	Asynchronous HTTP client
python-dotenv	Load environment variables
Gemini / OpenAI API	LLM for code generation
GitHub REST API	Repo creation + Pages deployment
AsyncIO	Task concurrency
Docker	Containerization for Hugging Face or local deploy
ğŸ§‘â€ğŸ’» Author

Pavan Kumar Yadav
ğŸ“˜ Project: TDS_P1_SEP_2025
ğŸŒ GitHub: [@PAVAN2005-LAB]((https://github.com/PAVAN2005-LAB)

-----
ğŸ§© Example Generated Project
Markdown â†’ HTML Converter (FastAPI + Hugging Face)

An example of an app generated by the LLM Builder & Deployer system.

ğŸ”— [Repo:](https://github.com/PAVAN2005-LAB/markdown-to-html-001)

ğŸŒ [Live Page](https://pavan2005-lab.github.io/markdown-to-html-001/)


This app converts Markdown text into HTML using FastAPI.
Itâ€™s lightweight, fast, and perfect for automation pipelines.


-----

ğŸ§  Features

Converts Markdown to HTML instantly

Simple FastAPI-based JSON API

Deployed automatically on Hugging Face

Fully open-source (MIT License)



------------
âš™ï¸ How It Works
@app.post("/convert")
async def convert_markdown(request: Request):
    data = await request.json()
    md_text = data.get("markdown", "")
    html_output = markdown.markdown(md_text)
    return JSONResponse(content={"html": html_output})


------
ğŸ“¥ Input:

{ "markdown": "# Hello World\nThis is a **bold** example." }

----
ğŸ“¤ Output:

{ "html": "<h1>Hello World</h1><p>This is a <strong>bold</strong> example.</p>" }

ğŸ§ª Try It Yourself
Swagger UI:

ğŸ”— https://pavan2005-lab-markdown-to-html-001.hf.space/docs

Via cURL:
curl -X POST https://pavan2005-lab-markdown-to-html-001.hf.space/convert \
  -H "Content-Type: application/json" \
  -d '{"markdown": "# Title\Bold Text"}'


----

also can deploy on hugging face to call API 



  â˜ï¸ Hugging Face Deployment Steps

1ï¸âƒ£ Go to Hugging Face Spaces
 â†’ Create a new Space
 
2ï¸âƒ£ Choose FastAPI as the template


3ï¸âƒ£ Upload:

main.py

requirements.txt

Dockerfile (optional for runtime customization)
4ï¸âƒ£ Click Deploy


5ï¸âƒ£ it will looks like  API will be live at:
ğŸ”— [https://pavan-yadav-sde-p1.hf.space/ready](https://pavan-yadav-sde-p1.hf.space/ready)



this repo, include mit lic so you can you it 


note: this file main python code also included spint of code which can expose your scerect key 

so, please remove that part(mention in main file itself )
